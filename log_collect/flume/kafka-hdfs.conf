# 测试命令 /usr/soft/hadoop-2.7.1/flume/bin/flume-ng agent -n a1 -c /usr/soft/hadoop-2.7.1/flume/conf -f /usr/soft/hadoop-2.7.1/flume/config/kafka-hdfs.conf -Dflume.root.logger=INFO,console
# 后台运行 nohup /usr/soft/hadoop-2.7.1/flume/bin/flume-ng agent -n a1 -c /usr/soft/hadoop-2.7.1/flume/conf -f /usr/soft/hadoop-2.7.1/flume/config/kafka-hdfs.conf 1>/dev/null 2>/dev/null &


a1.sources = r1
a1.sinks = k1
a1.channels = c1

## com.daoke360.common
a1.sources.r1.channels = c1
a1.sinks.k1.channel = c1




## sources config
a1.sources.r1.type = org.apache.flume.source.kafka.KafkaSource
a1.sources.r1.channels =c1
a1.sources.r1.batchSize = 5000
a1.sources.r1.batchDurationMillis = 2000
#kafka 节点地址
a1.sources.r1.kafka.bootstrap.servers =192.168.182.128:9092,192.168.182.129:9092,192.168.182.130:9092
a1.sources.r1.kafka.topics = event_log
#这里需要修改
a1.sources.r1.kafka.consumer.group.id = cbg1804


## channels config
a1.channels.c1.type = memory
a1.channels.c1.capacity = 100000000
a1.channels.c1.transactionCapacity = 10000000
a1.channels.c1.byteCapacityBufferPercentage = 60
a1.channels.c1.byteCapacity = 12800000000
a1.channels.c1.keep-alive = 60

#sinks config
a1.sinks.k1.type = hdfs
a1.sinks.k1.channel = c1
#这里需要修改
a1.sinks.k1.hdfs.path = hdfs://hadoop01:9000/logs/%Y/%m/%d
a1.sinks.k1.hdfs.fileType = DataStream
a1.sinks.k1.hdfs.filePrefix = flm_%H
a1.sinks.k1.hdfs.fileSuffix=.log
a1.sinks.k1.hdfs.minBlockReplicas=1
a1.sinks.k1.hdfs.rollInterval=60
a1.sinks.k1.hdfs.rollSize=0
a1.sinks.k1.hdfs.idleTimeout=0
a1.sinks.k1.hdfs.batchSize = 100
a1.sinks.k1.hdfs.rollCount=0
a1.sinks.k1.hdfs.useLocalTimeStamp = true

#a1.sinks.k1.channel = c1
#a1.sinks.k1.type = logger